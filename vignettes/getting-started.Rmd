---
title: "Getting Started with `nowcastNHSN`"
output:
  rmarkdown::html_vignette:
    code_folding: show
vignette: >
  %\VignetteIndexEntry{Getting Started with `nowcastNHSN`}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r httptest-setup, include = FALSE}
library(httptest)
start_vignette("getting-started")
```

```{r setup}
library(nowcastNHSN)
library(baselinenowcast)
library(ggplot2)
library(dplyr)
```

## Introduction

The `nowcastNHSN` package provides tools for fetching and doing inference with vintages of National Healthcare Safety Network (NHSN) hospitalisation incidence data for Covid-19, Influenza and RSV.
The goal of `nowcastNHSN` is to help with forecasting these signals in real-time, as new target data arrives in forecast hubs on a mid-week (Wednesday) schedule, by offering easy access to various methods of gathering historical reporting data and various approaches to nowcasting NHSN data.
This vignette demonstrates how to fetch NHSN reporting data and prepare it for nowcasting analysis using the `baselinenowcast` package.

**Forecasting hubs:**

- [COVID-19 Forecast Hub](https://github.com/CDCgov/covid19-forecast-hub)
- [FluSight Forecast Hub](https://github.com/cdcepi/FluSight-forecast-hub)
- [RSV Forecast Hub](https://github.com/CDCgov/rsv-forecast-hub)

## Data Fetching Overview

### Choosing a NHSN Data source

The primary function for fetching reporting data is `fetch_reporting_data()`, which dispatches on different data sources.
In this vignette, we'll focus on the `EpiData` source, which provides historical versions of NHSN hospital admission data maintained by the Delphi group at Carnegie Mellon University.

```{r create-source}
# Create a Delphi Epidata source for COVID-19 hospital admissions
source <- delphi_epidata_source(
  target = "covid",
  geo_types = "state"
)
```

## Fetching Reporting Data

In `nowcastNHSN`, we follow the forecast hub convention of using Saturdays as the reference date for weekly data.
We'll define the time period and locations we want to fetch data for:

```{r define-parameters}
# Define reference dates as an epirange (YYYYWW format)
epiweeks <- epidatr::epirange(202450, 202520)

reference_dates <- epiweeks

# For report dates, use "*" to get all available issue dates
# This will retrieve the full reporting history
report_dates <- epidatr::epirange(202450, 202525)

# Fetch data for specific states (lowercase for epidata API)
locations <- c("ca", "ny")
```

Now we can fetch the reporting data and filter out any report dates that are too far in the future:

```{r fetch-data, message = FALSE}
# Fetch the data
reporting_data <- fetch_reporting_data(
  source = source,
  reference_dates = reference_dates,
  report_dates = "*",
  locations = locations
)
```

```{r show-data}
# View the first few rows
head(reporting_data)
```

The returned data frame contains columns:

- `reference_date`: The Saturday ending the week when events occurred
- `report_date`: The Saturday when the data was reported
- `location`: Geographic identifier (state abbreviation or "US")
- `count`: Number of **new** COVID-19 hospital admissions reported on that report date (incremental, not cumulative)
- `signal`: The epidata signal name

Note that `fetch_reporting_data()` automatically converts the cumulative counts from the API to incremental counts, which is the format expected by `baselinenowcast::as_reporting_triangle()`.
For visualization of how vintages evolve over time, we'll also compute cumulative totals:

### Visualizing Reporting Delays

A feature of reporting data is that counts for the same reference date can change as new reports come in.
This backfilling of the data is a common challenge in using the latest data for real-time forecasting.
To visualize this, we first compute cumulative totals from the incremental counts, then plot the time series of Californian Covid-19 hospitalisations at different report dates:

```{r plot-delays, fig.width = 7, fig.height = 5, class.source = 'fold-hide'}
# Compute cumulative totals for visualization of vintages
# (sum of incremental counts up to each report date)
reporting_data_cumulative <- reporting_data |>
  arrange(location, reference_date, report_date) |>
  group_by(location, reference_date) |>
  mutate(cumulative_count = cumsum(count)) |>
  ungroup()

# Select a few report dates to compare
# Pick 5 evenly spaced report dates to show evolution over time
all_report_dates <- sort(unique(reporting_data_cumulative$report_date[reporting_data_cumulative$location == "ca"]))
selected_report_dates <- all_report_dates[
  seq(1, length(all_report_dates), length.out = 5) |> round()
]

selected_reports <- reporting_data_cumulative |>
  filter(
    location == "ca",
    report_date %in% selected_report_dates
  )

# Create the plot
ggplot(selected_reports, aes(x = reference_date, y = cumulative_count, color = as.factor(report_date))) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  scale_y_log10() +
  labs(
    title = "COVID-19 Hospital Admissions in California",
    subtitle = "How reported counts change as data is updated (cumulative totals)",
    x = "Reference Date (Week Ending)",
    y = "Confirmed Admissions (log scale)",
    color = "Report Date"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_color_brewer(palette = "Set1")
```

This plot shows how the same reference dates can have different reported counts depending on when the data was reported.
Counts for reference dates that are recent to their report date may be incomplete, and typically get revised upward in subsequent reports.

### Converting to a Reporting Triangle

To perform nowcasting analysis, we need to convert the fetched reporting data into a reporting triangle format, which organizes counts by reference date and delay.
We can use the `baselinenowcast::as_reporting_triangle()` function to do this.
The data from `fetch_reporting_data()` is already in the long format required by `baselinenowcast::as_reporting_triangle()`.
Note that the reporting triangle is created for a single location at a time:

```{r create-triangle, message = FALSE}
# Filter to a single location (California) for the reporting triangle
ca_data <- reporting_data |>
  filter(location == "ca")

nowcast_date <- max(ca_data$reference_date)
ca_data <- ca_data |>
  filter(report_date <= nowcast_date)

# Convert to reporting triangle format
reporting_triangle <- as_reporting_triangle(
  ca_data,
  delays_unit = "weeks"
) |>
  truncate_to_delay(max_delay = 15)

# View the reporting triangle structure
print(reporting_triangle)
```

The reporting triangle is now ready for nowcasting analysis using the `baselinenowcast` package.

### Heatmap View of Reporting Triangle

A heatmap provides another useful way to visualize the reporting triangle structure, showing how counts evolve across different combinations of reference and report dates:

```{r plot-heatmap, fig.width = 8, fig.height = 6, class.source = 'fold-hide'}
# Extract data from reporting triangle for visualization
rt_data <- reporting_triangle |>
  as.data.frame() |>
  mutate(
    # Convert delay back to report_date for the heatmap
    report_date = reference_date + delay * 7  # delay is in weeks
  )

ggplot(rt_data, aes(x = reference_date, y = report_date, fill = count)) +
  geom_tile(color = "white", linewidth = 0.5) +
  scale_fill_viridis_c(option = "plasma", na.value = "grey90") +
  labs(
    title = "Reporting Triangle Heatmap - California COVID-19 Admissions",
    subtitle = "Each cell shows the count reported for a reference date as of a report date",
    x = "Reference Date (Week Ending)",
    y = "Report Date (Week Ending)",
    fill = "Count"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank()
  )
```

In this heatmap, the diagonal represents the most recent data available at each time point, while values below the diagonal show how historical data has been revised upward through backfill.

## Using `baselinenowcast` for Nowcasting

The `baselinenowcast` package provides a simple baseline model for nowcasting reporting triangles; [see here for details](https://baselinenowcast.epinowcast.org/articles/model_definition.html).
Now that we have our reporting triangle, we can use it to generate nowcasts that estimate the final count for recent reference dates that are likely still under-reported.

### Fitting the Baseline Nowcast Model

The baseline nowcast model fits a simple delay distribution to the reporting triangle and uses it to predict the expected final count.
We can fit the model and generate predictions as follows:

```{r fit-nowcast, message = FALSE}
# Fit the baseline nowcast model
nowcast_fit <- baselinenowcast(reporting_triangle, draws = 100)

```

### Extracting Nowcast Predictions

The nowcast object contains predictions for each reference date.
We can extract these predictions and compare them to the latest reported counts:

```{r extract-predictions, message = FALSE}
# Extract the nowcast predictions by summarizing the draws
nowcast_predictions <- nowcast_fit |>
  group_by(reference_date) |>
  summarise(
    median = median(pred_count),
    q5 = quantile(pred_count, 0.05),
    q95 = quantile(pred_count, 0.95),
    .groups = "drop"
  )

# The nowcast predicts the FINAL CUMULATIVE count for each reference date.
# We need to compute cumulative totals from the incremental data for comparison.
latest_cumulative <- ca_data |>
  arrange(reference_date, report_date) |>
  group_by(reference_date) |>
  summarise(
    # Sum all incremental counts to get cumulative total as of latest report
    latest_count = sum(count),
    .groups = "drop"
  )

comparison <- nowcast_predictions |>
  left_join(latest_cumulative, by = "reference_date") |>
  mutate(
    reporting_completeness = latest_count / median * 100
  )

# View the comparison
print(comparison |> select(reference_date, latest_count, median, q5, q95, reporting_completeness))
```

### Visualizing Nowcast Results

We can visualize the nowcast results by plotting the predicted final counts alongside the latest reported counts:

```{r plot-nowcast, fig.width = 8, fig.height = 5, class.source = 'fold-hide'}
ggplot(comparison, aes(x = reference_date)) +
  # Latest reported counts
  geom_point(aes(y = latest_count, color = "Latest Reported"), size = 3) +
  geom_line(aes(y = latest_count, color = "Latest Reported"), linewidth = 1) +
  # Nowcast median
  geom_point(aes(y = median, color = "Nowcast (Median)"), size = 3, shape = 17) +
  geom_line(aes(y = median, color = "Nowcast (Median)"), linewidth = 1, linetype = "dashed") +
  # Nowcast uncertainty
  geom_ribbon(aes(ymin = q5, ymax = q95, fill = "90% Prediction Interval"), alpha = 0.3) +
  labs(
    title = "Baseline Nowcast for California COVID-19 Admissions",
    subtitle = "Comparing latest reported counts with nowcast predictions",
    x = "Reference Date (Week Ending)",
    y = "Confirmed Admissions",
    color = "Count Type",
    fill = NULL
  ) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_color_manual(values = c("Latest Reported" = "#E41A1C", "Nowcast (Median)" = "#377EB8")) +
  scale_fill_manual(values = c("90% Prediction Interval" = "#377EB8"))
```

This plot shows that for recent reference dates, the nowcast predictions (dashed line with triangles) are higher than the latest reported counts (solid line with circles), indicating expected under-reporting.
The shaded area represents the 90% prediction interval, showing the uncertainty in the nowcast.

### Understanding Reporting Completeness

We can also visualize the estimated reporting completeness over time to identify which reference dates are most affected by reporting delays:

```{r plot-completeness, fig.width = 8, fig.height = 5, class.source = 'fold-hide'}
ggplot(comparison, aes(x = reference_date, y = reporting_completeness)) +
  geom_line(linewidth = 1, color = "#4DAF4A") +
  geom_point(size = 3, color = "#4DAF4A") +
  geom_hline(yintercept = 100, linetype = "dashed", color = "grey50") +
  labs(
    title = "Estimated Reporting Completeness Over Time",
    subtitle = "Percentage of final count already reported (latest count / nowcast median)",
    x = "Reference Date (Week Ending)",
    y = "Reporting Completeness (%)"
  ) +
  scale_y_continuous(limits = c(0, 110), breaks = seq(0, 100, 20)) +
  theme_minimal()
```

This completeness plot shows that recent reference dates typically have lower reporting completeness, as expected.
For forecasting applications, the nowcast estimates can be used to adjust for this expected under-reporting when generating predictions.

## Custom Uncertainty Models

The `baselinenowcast` package makes point estimates for the predicted eventual reports on data references dates.
For uncertainty quantification, `baselinenowcast` also makes dispersion estimates from retrospective data using the point estimates as fixed means.
This also allows us to sample reference date/lag realisations from the nowcast model using a parametric distribution, which by default is a negative binomial.
However, `nowcastNHSN` provides alternative uncertainty models that may be more appropriate for different data characteristics.
In particular, sometimes we see down revisions to NHSN data which has probability zero under a negative binomial model.
`nowcastNHSN` provides dispersion fits and sampling for two distributions that can sample negative integer values: the rounded normal and the Skellam distributions.

### Using Normal Distribution for Uncertainty

The normal distribution (rounded to integers) provides a simple symmetric uncertainty model.
We use `fit_by_horizon()` from baselinenowcast with our custom `fit_normal` function:
```{r nowcast-normal, message = FALSE}
# Nowcast using normal distribution for uncertainty
nowcast_normal <- baselinenowcast(
  reporting_triangle,
  draws = 100,
  uncertainty_model = function(obs, pred) {
    fit_by_horizon(obs, pred, fit_model = fit_normal)
  },
  uncertainty_sampler = sample_normal
)

# Summarize the normal nowcast
normal_summary <- nowcast_normal |>
  group_by(reference_date) |>
  summarise(
    median = median(pred_count),
    q5 = quantile(pred_count, 0.05),
    q95 = quantile(pred_count, 0.95),
    .groups = "drop"
  ) |>
  mutate(model = "Normal")

print(normal_summary)
```

### Using Skellam Distribution for Uncertainty

The Skellam distribution is the difference of two Poisson random variables, making it naturally suited for count data.
It produces integer samples directly and can handle cases where deviations might be negative:

```{r nowcast-skellam, message = FALSE}
# Nowcast using Skellam distribution for uncertainty
nowcast_skellam <- baselinenowcast(
  reporting_triangle,
  draws = 100,
  uncertainty_model = function(obs, pred) {
    fit_by_horizon(obs, pred, fit_model = fit_skellam)
  },
  uncertainty_sampler = sample_skellam
)

# Summarize the Skellam nowcast
skellam_summary <- nowcast_skellam |>
  group_by(reference_date) |>
  summarise(
    median = median(pred_count),
    q5 = quantile(pred_count, 0.05),
    q95 = quantile(pred_count, 0.95),
    .groups = "drop"
  ) |>
  mutate(model = "Skellam")

print(skellam_summary)
```

### Comparing Uncertainty Models

Let's visualize how the different uncertainty models compare:

```{r plot-uncertainty-comparison, fig.width = 8, fig.height = 5, class.source = 'fold-hide'}
# Combine summaries
all_summaries <- rbind(
  normal_summary,
  skellam_summary
)

ggplot(all_summaries, aes(x = reference_date)) +
  # Latest reported counts
  geom_point(
    data = comparison,
    aes(y = latest_count, color = "Latest Reported"),
    size = 3
  ) +
  geom_line(
    data = comparison,
    aes(y = latest_count, color = "Latest Reported"),
    linewidth = 1
  ) +
  # Nowcast uncertainty ribbons
  geom_ribbon(
    data = normal_summary,
    aes(ymin = q5, ymax = q95, fill = "Normal"),
    alpha = 0.3
  ) +
  geom_ribbon(
    data = skellam_summary,
    aes(ymin = q5, ymax = q95, fill = "Skellam"),
    alpha = 0.3
  ) +
  # Nowcast medians (use normal_summary since medians are the same)
  geom_point(
    data = normal_summary,
    aes(y = median, color = "Nowcast (Median)"),
    size = 3, shape = 17
  ) +
  geom_line(
    data = normal_summary,
    aes(y = median, color = "Nowcast (Median)"),
    linewidth = 1, linetype = "dashed"
  ) +
  labs(
    title = "Comparison of Uncertainty Models",
    subtitle = "Comparing Normal vs Skellam prediction intervals",
    x = "Reference Date (Week Ending)",
    y = "Confirmed Admissions",
    color = "Count Type",
    fill = "90% Prediction Interval"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_color_manual(values = c("Latest Reported" = "#E41A1C", "Nowcast (Median)" = "#377EB8")) +
  scale_fill_manual(values = c("Normal" = "#377EB8", "Skellam" = "#4DAF4A"))
```

Note that both custom sampling distributions are capable of sampling down-revisions, albeit with different dispersion.
The Skellam distribution is constrained to have higher variance than mean so is probably a better model for situations with low counts and fairly common down-revisions.
The normal distribution (rounded) probably works better for larger counts with rare down-revisions.

```{r httptest-teardown, include = FALSE}
end_vignette()
```
